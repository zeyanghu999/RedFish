{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1508a533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import sys,os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c685285d",
   "metadata": {},
   "source": [
    "# 神经网络的学习\n",
    "* 本章将介绍神经网络的学习，即利用数据决定参数值的方法\n",
    "* 用Python实现对MNIST手写数字数据集的学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d0f005",
   "metadata": {},
   "source": [
    "# 从数据中学习\n",
    "如果让我们自己来设计一个能将5正确分类的程序，就会意外地发现这是一个很难的问题。\n",
    "一种方案是，先从图像中提取特征量，再用机器学习技术学习这些特征量的模式。\n",
    "* 特征量是指从输入数据（输入图像）中准确地提取本质数据（重要的数据）的转换器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4831a916",
   "metadata": {},
   "source": [
    "两种针对机器学习任务的方法，如下图所示\n",
    "## 第一种需要人为干涉\n",
    "## 第二种不需要人为干涉\n",
    "\n",
    "![两种针对机器学习任务的方法](img\\img4_1.png \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dcac90",
   "metadata": {},
   "source": [
    "# 均方误差（mean squared error）\n",
    "均方误差会计算神经网络的输出和正确解监督数据的各个元素之差的平方,再求总和\n",
    "![均方误差](img\\img4_2.png \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44906940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c4f6f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09750000000000003"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设“2”为正确解\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "mean_squared_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa39ded0",
   "metadata": {},
   "source": [
    "# 交叉熵误差（cross entropy error）\n",
    "交叉熵误差的值是由正确解标签所对应的输出结果决定的\n",
    "![交叉熵误差](img\\img4_3.png \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73958316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7 # 当出现np.log(0)时，np.log(0)会变为负无限大的-inf\n",
    "                # 作为保护性对策，添加一个微小值可以防止负无限大的发生\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4b12cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设“2”为正确解\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38496a7d",
   "metadata": {},
   "source": [
    "    均方误差和交叉熵误差是两种不同的损失函数，损失函数是表示神经网络性能的“恶劣程度”的指标，即当前的神经网络对监督数据在多大程度上不拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7f011a",
   "metadata": {},
   "source": [
    "# mini-batch学习\n",
    "从训练数据中选出一批数据（称为mini-batch,小批量），然后对每个mini-batch进行学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919ba6ef",
   "metadata": {},
   "source": [
    "## 批数据的损失函数\n",
    "以交叉熵误差为例：\n",
    "![交叉熵误差对于批数据的损失函数](img\\img4_4.png \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad40aca",
   "metadata": {},
   "source": [
    "# 数值微分\n",
    "数值微分就是求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c794afea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#求一个函数的导数\n",
    "def numerical_diff(f , x):\n",
    "    h = 1e-4 #0.0001\n",
    "    return (f(x+h) - f(x-h)) / (2*h)\n",
    "#这里使用的方法是导数的定义"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a32ad74",
   "metadata": {},
   "source": [
    "例子，求一个二次函数的导数，如下：\n",
    "![二次函数](img\\img4_5.png \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24dee565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afe9bfe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#画出这个函数图像\n",
    "x = np.arange(0.0, 20.0, 0.1) # 以0.1为单位，从0到20的数组x\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a7f0eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1999999999990898"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#计算一下这个函数在x = 5处的导数\n",
    "numerical_diff(function_1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d1a42b",
   "metadata": {},
   "source": [
    "## 偏导数\n",
    "求偏导数时可以将其中一项固定，把原函数转化为只有一个自变量，然后对这个自变量正常求导即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b9f752",
   "metadata": {},
   "source": [
    "## 梯度（gradient）\n",
    "由全部变量的偏导数汇总而成的向量称为梯度（gradient）。\n",
    "* 梯度就是一次求出所有的偏导数，然后把它们用一个向量组表示\n",
    "\n",
    "梯度的实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e650b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f ,x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x) #默认x为numpy数组，这个函数可以生成一个与x形状一样，元素全为0的数组\n",
    "    for idx in range(x.size):\n",
    "        tmp_val= x[idx] #把x值保存下来\n",
    "        #f(x+h)\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        #f(x-h)\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2)/(2*h)\n",
    "        x[idx]=tmp_val # 还原x的值\n",
    "    return grad\n",
    "# 这样就一次求了两个自变量的偏导数并且返回一个数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b6f0ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 8.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#给出一个函数，检查梯度的计算有没有出错\n",
    "#此处的函数是f(x0 + x1)=x0**2 + x1**2\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "numerical_gradient(function_2, np.array([3.0, 4.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d5ca04",
   "metadata": {},
   "source": [
    "![梯度的解释](img\\img4_51.png \" \")\n",
    "\n",
    "虽然图 4-9 中的梯度指向了最低处，但并非任何时候都这样。实际上，\n",
    "梯度会指向各点处的函数值降低的方向。更严格地讲，梯度指示的方向\n",
    "是**各点处的函数值减小最多的方向** ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b40a67",
   "metadata": {},
   "source": [
    "# 梯度法\n",
    "* 目的：求出损失函数最小值。因为损失函数很复杂，参数空间庞大，我们不知道它在何处能取得最小值。而通过巧妙地使用梯度来寻找函数最小值（或者尽可能小的值）的方法就是梯度法。\n",
    "* 方法：在梯度法中，函数的取值从当前位置沿着梯度方向前进一定距离，然后在新的地方重新求梯度，再沿着新梯度方向前进，如此反复，不断地沿梯度方向前进。\n",
    "\n",
    "**这里需要注意的是，梯度表示的是各点处的函数值减小最多的方向。** 因此，\n",
    "无法保证梯度所指的方向就是函数的最小值或者真正应该前进的方向。实际\n",
    "上，在复杂的函数中，梯度指示的方向基本上都不是函数值最小处。\n",
    "![数学式表示梯度法](img\\img4_6.png \" \")\n",
    "\n",
    "η表示更新量，在神经网络的学习中，称为学习率（learning rate）。\n",
    "\n",
    "学习率需要事先确定为某个值，比如0.01或0.001。一般而言，这个值过大或过小，都无法抵达一个“好的位置”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aae11eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#这是梯度法的简单python实现\n",
    "# f-函数  init_x-初始x值  lr-学习率  step_num-循环的次数\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x) #每次都求一个新的梯度\n",
    "        x -= lr * grad #沿着新的梯度前进\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071ddd41",
   "metadata": {},
   "source": [
    "上面这个函数就可以求函数的极小值，顺利的话还可以求最小值。\n",
    "\n",
    "现在解决下面的这个问题\n",
    "\n",
    "求`f(x0 + x1)=x0**2 + x1**2`的最小值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bec6a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.62962878e-09, 1.83333238e-09])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([8.0,9.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)\n",
    "#真实的最小值就是(0,0)，求出来的值也非常接近它"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700e8d06",
   "metadata": {},
   "source": [
    "下面，我们以一个简单的神经网络为例，来实现求梯度的代码。\n",
    "\n",
    "该神经网络的权重是2 X 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ed98221",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#实现求梯度的代码\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 用高斯分布进行初始化\n",
    "    \n",
    "    def predict(self, x): #用于预测 \n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self,x,t): #用于求损失函数\n",
    "        z = self.predict(x)#x接收输入数据\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y,t)#t接收正确解标签\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65c8fcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.18543619  0.46651284 -0.02628108]\n",
      " [ 0.64824882 -0.77673585  0.56865545]]\n"
     ]
    }
   ],
   "source": [
    "#用一下这个simplenet\n",
    "net = simpleNet()\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcb616da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.47216222 -0.41915456  0.49602125]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.6 , 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a78c719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8657840727376378"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p)#最大值的索引\n",
    "\n",
    "t = np.array([0, 0, 1]) # 正确解标签\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9552d553",
   "metadata": {},
   "source": [
    "## 接下来求梯度\n",
    "\n",
    "唉我都不想记了，记在笔记本上咯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f876267",
   "metadata": {},
   "source": [
    "## 流程以外的东西\n",
    "\n",
    "**lambda表示法**：Lambda表示法是一种函数定义的方式，它是一种匿名函数，即没有函数名的函数定义。\n",
    "\n",
    "在lambda表示法中，函数定义以lambda关键字开头，后面跟着参数列表和函数体，用冒号隔开\n",
    "\n",
    "比方说：\n",
    "\n",
    "`lambda arguments: expression `\n",
    "\n",
    "其中，arguments表示参数列表，可以是一个或多个参数，用逗号隔开；expression表示函数体，是一个表达式，可以是任何可求值的Python表达式。\n",
    "\n",
    "例如，下面的代码定义了一个lambda函数，用于计算两个数的和：\n",
    "\n",
    "`add = lambda x, y: x + y`\n",
    "\n",
    "这个lambda函数可以通过add(2, 3)的方式调用，返回结果为5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5756a5b4",
   "metadata": {},
   "source": [
    "# 大的要来啦---两层神经网络的类的实现\n",
    "先把代码粘出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c458907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 初始化权重\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        return y\n",
    "    \n",
    "    # x:输入数据, t:监督数据\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads\n",
    "\n",
    "    #这些内容在书本pdf第138页，建议常常复习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0459a7c",
   "metadata": {},
   "source": [
    "## 首先是__init__(self, input_size, hidden_size, output_size, weight_init_std=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edf7e5a",
   "metadata": {},
   "source": [
    "* 进行了一些初始化的设定\n",
    "* 形参列表依次为self，输入层神经元个数，隐藏层神经元个数，输出层神经元个数，步长\n",
    "* 解释一下w：在这个例子中，权重矩阵W1和W2的初始值是从标准正态分布中随机生成的，并且乘以了一个小的标量weight_init_std，以缩小它们的值域。这样做是为了确保权重矩阵的初始值不会过大或过小，从而避免出现梯度消失或爆炸的情况。\n",
    "* 解释一下b：而偏置向量b1和b2的初始值设置为全零向量，是因为在神经网络中，偏置的作用是将神经元的激活值平移一定量，使得神经元可以更好地适应数据。因此，偏置项的初始值通常设置为零，可以让神经元的激活值在开始时偏向正值或负值，从而增强了神经元的表达能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29d9fd1",
   "metadata": {},
   "source": [
    "## 其次是predict(self, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4a73dd",
   "metadata": {},
   "source": [
    "* 这是一个预测函数\n",
    "\n",
    "* w1 w2,b1 b2分别是第一层和第二层的权重和偏置\n",
    "\n",
    "* 输入层到隐藏层之间使用向量组的点乘，然后加上偏置\n",
    "\n",
    "* 隐藏层的内部使用**sigmoid函数**当作激活函数\n",
    "\n",
    "* 隐藏层再与它的权重点乘，再加上偏置得到输出层的初始值\n",
    "\n",
    "* 输出层使用**softmax函数**当作激活函数，将初始的数值转化为概率的分布\n",
    "\n",
    "* **softmax函数**和**sigmoid函数**的区别在于：在二分类问题中，**sigmoid函数**通常作为输出层激活函数使用；在多分类问题中，**softmax函数**通常作为输出层激活函数使用，将网络的输出转换为各个类别的概率分布"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96de223c",
   "metadata": {},
   "source": [
    "## 然后是loss(self, x, t)\n",
    "\n",
    "* 这个方法的作用就是计算当前的x和t的损失函数值是多少\n",
    "* 思路是：先通过预测函数预测出结果，再通过预测与真正结果的交叉熵得到数值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2ce1dc",
   "metadata": {},
   "source": [
    "## 然后是accuracy(self, x, t)\n",
    "\n",
    "* 这个函数的作用是计算预测成功的概率\n",
    "* 思路是：我先预测出结果，然后用正确的解标签数目/总数目，得到一个概率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495817b5",
   "metadata": {},
   "source": [
    "## 然后是numerical_gradient(self, x, t)\n",
    "\n",
    "* 这个函数用来计算当前损失函数(在自变量为W时)的偏导数\n",
    "* 使用数值微分法计算，简单粗暴\n",
    "* 这里用了**函数重载**\n",
    "* 别看函数体里有numerical_gradient()，但是和它自己的形参列表不同\n",
    "* 函数体里的那个numerical_gradient()是import进来的另外的函数\n",
    "\n",
    "~~我真是个蠢货一开始还以为是递归我说我怎么哪哪看不懂还以为自己脑子坏了~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c65b166",
   "metadata": {},
   "source": [
    "## 最后是gradient(self, x, t)\n",
    "* 这个函数还没搞懂，已知使用了另一种方法求梯度(偏导数)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5ca384",
   "metadata": {},
   "source": [
    "# mini-batch的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e43c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "from other.two_layer_net import TwoLayerNet\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) =load_mnist(normalize=True, one_hot_label = True)\n",
    "# x_train的形状是(60000,784)\n",
    "\n",
    "train_loss_list = [] #训练函数的损失函数值列表，盲猜一会儿会生成这个列表的图像\n",
    "\n",
    "# 超参数\n",
    "iters_num = 10000 #循环次数\n",
    "train_size = x_train.shape[0] #train_size是60000，int型\n",
    "batch_size = 100 #批大小\n",
    "learning_rate = 0.1 #步长\n",
    "\n",
    "#网络初始化\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 获取mini-batch\n",
    "    batch_mask = np.random.choice(train_size, batch_size)#这是从训练数据中随机抽出一批次数据的写法\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]#这两步挑选出了需要的数据\n",
    "    \n",
    "    #计算梯度\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    \n",
    "    #更新参数\n",
    "    #这里就真正凸显出SGD了\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 记录学习过程\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)#这里使用append方法记录函数值到列表里，前面猜对了\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316347ca",
   "metadata": {},
   "source": [
    "小小科普！\n",
    "\n",
    "np.random.choice函数是NumPy库中用于从给定数组中随机抽取元素的函数。它的语法如下：\n",
    "\n",
    "`numpy.random.choice(a, size=None, replace=True, p=None)`\n",
    "\n",
    "参数说明：\n",
    "\n",
    "a：要从中抽取元素的数组，可以是一维数组或整数。\n",
    "\n",
    "size：抽取的样本数量，默认为1。如果为整数，则返回抽取元素的一维数组；如果为元组，则返回抽取元素的多维数组。\n",
    "\n",
    "replace：是否可以重复抽取同一个元素，取值为True或False，默认为True。\n",
    "\n",
    "p：数组中每个元素被抽取的概率，可以是一维数组或标量。如果不指定，则默认为每个元素被抽取的概率相等。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2426016e",
   "metadata": {},
   "source": [
    "## 以下是更加先进的两层神经网络的学习过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1db4d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_laobel = True)\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 平均每个epoch的重复次数。就是整个训练数据的重复次数\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "# 超参数\n",
    "iters_num = 10000\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 获取mini-batch。随机抽出一批数据\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 计算梯度\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    # grad = network.gradient(x_batch, t_batch) # 高速版!\n",
    "    \n",
    "    # 更新参数\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "        loss = network.loss(x_batch, t_batch)\n",
    "        train_loss_list.append(loss)\n",
    "        # 计算每个epoch的识别精度\n",
    "        if i % iter_per_epoch == 0:\n",
    "            train_acc = network.accuracy(x_train, t_train)#训练数据的识别精度\n",
    "            test_acc = network.accuracy(x_test, t_test)#测试数据的识别精度\n",
    "            train_acc_list.append(train_acc)\n",
    "            test_acc_list.append(test_acc)\n",
    "            print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2221b320",
   "metadata": {},
   "source": [
    "第一步：从数据集中提取数据\n",
    "\n",
    "第二步：设定列表来记录每次训练完成之后各个参数的结果\n",
    "\n",
    "第三步：设定超参数\n",
    "\n",
    "第四步：构建神经网络\n",
    "\n",
    "第五步：进入循环；抽取第一批数据\n",
    "\n",
    "第六步：计算梯度\n",
    "\n",
    "第七步：向着梯度下降的方向，更新参数\n",
    "\n",
    "第八步：将损失函数值存入损失训练函数列表中\n",
    "\n",
    "第九步：计算训练数据的识别精度和测试数据的识别精度，然后分别存入列表中"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-my_env]",
   "language": "python",
   "name": "conda-env-.conda-my_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
